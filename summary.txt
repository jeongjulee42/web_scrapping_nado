Xpath - html에서 element를 지칭하기 위해 사용
다음과 같이 사용
1. //*[@class = "className"]  -  모든 문서에서 찾기
2. /html/body/div[@id="idName"]
브라우저가 다 찾아준다.

====================================================================

requests - html 문서 정보 가져오기
import requests

우리가 원하는 url에 접속해서 정보 가져오기
res = requests.get("http://google.com")

정보를 가져오는 곳에서 문제가 없는지 확인
res.status_code와 같은것을 써도 된다
res.raise_for_status() -> 정상적으로 문서를 가져오면 문제x , 그렇지 않은 경우 에러 발생
print(res.text) -> 가져온 html 문서를 확인한다.

====================================================================

정규식
import re 를 통해 사용
ca?e 와 같은것 검색
먼저 패턴값을 받기 위해
p = re.compile("ca.e")
. (ca.e)->하나의 문자를 의미     care, cafe, case 는 가능 | caffe 이런것은 불가능
^ (^de)->문자열의 시작을 의미    de로 시작하는 모든것이 매칭된다.
$ (se$)-> 문자열의 끝을 의미     case, ... 과 같이 se로 끝나는 모든것이 매칭이된다.
정규식과 메칭이 되는지 출력해주는 코드, 매칭 안되면 에러 발생
단 match는 주어진 문자열의 처음부터 일치하는지 확인한다.
m = p.match("case")
print(m.group())

m.group()
group: 일치하는 문자열 반환
m.string()
string: 입력받은 문자열 반환
m.start()
start: 일치하는 문자열의 시작 인덱스 반환
m.end()
end: 일치하는 문자열의 끝 인덱스 반환
m.span()
span: 일치하는 문자열의 시작과 끝 인덱스 반환

p.search("_____")
search: 주어진 문자열 중에 일치하는게 있는지 확인
p.findall("careless")
findall: 일치하는 모든것을 리스트 형태로 변환, 문장 내에서 일치하는것이 여러개이면 리스트 형태로 전부 반환

정규식 정리
1. p = re.compile("원하는 형태")
2. m = p.match("비교할 문자열") : 주어진 문자열의 처음부터 일치하는지 확인
3. m = p.search("비교할 문자열") : 주어진 문자열 중에 일치하는게 있는지 확인
4. lst = p.findall("비교할 문자열") : 일치하는 모든 것을 리스트 형태로 반환

====================================================================

userAgent
pc, 스마트폰의 접속에 따라 화면이 바뀌는 것처럼 웹에서는 사용자의 정보를 알수있다.
브라우저가 웹사이트에 접속할때 주는 헤더 정보를 통해 웹사이트는 어떠한 정보를 제공할지 선택한다.

웹크롤링과 스크래핑의 경우 웹사이트는 컴퓨터가 무단으로 정보를 긁어가는것을 방지하기 위해 사용자를 차단할수 있는데,
이것을 방지하기 위해 유저에이전트를 사용한다.
구글에 userAgent string를 검색하여 나의 접속 정보를 알아내자. (크롬, 사파리같이 다른 브라우저를 사용하면 유저에이전트가 달라진다)

다음과 같이 사용.
url = "http://naver.com"
headers = {
    "User-Agent": "나의 접속 정보"
}
res = requests.get(url, headers = headers)


====================================================================

Beautiful Soup 사용

requests와 같이 사용
from bs4 import BeautifulSoup
soup = BeautifulSoup(res.text, "lxml")
우리가 가져온 html 문서를 lxml를 사용하여 bs 객체로 만든다.
soup를 통해 element에 접근한다.
ex) soup.title - title정보 가져오기
ex) soup.title.get_text() - title 정보에서 글자만 뺴오기
ex) soup.a - 첫번째로 발견된 a 엘리먼트 반환
ex) soup.a.attrs - a태그의 속성 정보 출력
ex) soup.a["href"] - href 속성 값 정보 출력
ex) soup.find("a", attrs={"class":"class_name"}) - soup 객체 내의 a태그를 가지며, class가 class_name인 것에 대해 찾기
ex) soup.find(attrs={"class":"class_name"}) - a 태그가 없어도 찾을수있다.

# soup에서 부모, 자식, 형제 관계 사용하기

rank1 = soup.find(~~)
rank2 = rank1.next_sibling - 다음 엘리먼트로 넘어가기
* 태그 사이에 줄바꿈 있는 경우가 있을수 있다. 이 경우 2번 연속 써주면 된다.
rank1 = rank2.previous_sibling - 이전 엘리먼트로 넘어가기
rank_parent = rank1.parent - 부모 엘리먼트로 넘어가기
rank_parent.a.get_text() - 자식 엘리먼트에서 텍스트 추출하기

* 태그 사이에 줄바꿈 때문에 next_sibling을 몇번 써야할지 모르겠을때
rank2 = rank1.find_next_sibling("조건 ex> li") - 조건에 해당하는것만 찾기
단 이경우 조건에 맞는 형제들을 모두 가져온다.

텍스트값을 비교하여 해당하는것 찾기
soup.find("a", text="text_var")

해당하는 모든 항목 가져오기
find_all 사용, find는 첫 엘리먼트만 가져온다.
find_all을 사용하는 경우 리스트로 반환된다.
** find_all("li",attrs={"class": re.compile("____")}) 와 같이 정규식을 쓸수도 있다.

찾는 방법
soup.find().find().find_all()

여러개 찾기
attrs={"class": ["ULeU3b neq64b","  " ]}
클래스가 이 리스트안에있는것들과 일치하는것들을 가져온다

여러개 찾기 2
attrs={"class":"ULeU3b neq64b", "id":"id_name"}

여러개 찾기 3
(attrs={"class":"ULeU3b neq64b", "id":"id_name"}, text="text_comment")
텍스트도 함께 찾기

**
find_all("li", limit=3)
3개만 가져오기

*** http method
http 서버에 우리가 요청을 보내면 요청에 맞는 응답을 준다.
이떄 요청에 포함되는것중 http method가 있는데 두가지 방식이 있다.
get & post

get - 요청 정보를 누구나 볼수있도록 url에 넣어서 보내는것.
ex) https://www.coupang.com/np/search?minPrice=1000&maxPrice=100000&page=1 
? 뒤에 있는것부터 변수 = 값 의 형태로 우리가 이것을 바꾸어 쉽게 요청할수도 있다.
but get은 한번 보낼때 보낼수 있는 데이터의 양에 한계가 있다.

post - 요청 정보를 누구나 볼수 없도록 url이 아닌 http message body에 숨겨 보내는 방식
페이지는 변해도 url이 그대로이다. 보낼수 있는 데이터의 양에 한계가 없다.

====================================================================

Selenium
웹페이지를 자동화 할수있는 프레임 워크
이것을 통해 직접 웹브라우저를 컨트롤하며 html을 가져와 스크래핑 한다.
from selenium import webdriver

해당 브라우저의 버전에 맞는 웹드라이버를 설치해야한다.
browser = webdriver.Chrome("경로")
browser.get("url") - 객체를 생성하여 해당 url로 이동

엘리먼트 찾아가기 - elem = browser.find_element_by_class_name("class_name")
클릭 - elem.click()
이전페이지로 이동 - browser.back()
앞으로 이동 - browser.forward()
새로고침 - browser.refresh()

값 전달 ( ex)검색창에 값 넣기) - elem.send_keys("전달할 값")

엔터 누르기
from selenium.webdriver.common.keys import Keys
elem.send_keys(Keys.ENTER)

테그로 정보찾기
하나 찾기 - elem = browser.find_element_by_tag_name("a")
모두 찾기 - elem = browser.find_elements_by_tag_name("a")

정보 찾기
단일 elem 접근 -  elem = browser.find_element_by_name("q")
Xpath로 찾기 - elem = browser.find_element_by_xpath("xpath")
** 이떄 " 중복시 '를 사용한다.





창 닫기
현재 떠있는 창만 종료 - browser.close()
탭 모두 종료 - browser.quit()

창 최대화
browser.maximize_window()

속성값 찾기 - e.get_attribute("href")

id, pw 같이 입력하는데에 있어 기존에 남아있는 데이터 지우기
browser.find_element().clear()

지금 페이지의 html 문서를 전부 출력하기 - browser.page_source

셀레니움에서 Xpath 사용
from selenium.webdriver.common.by import By 
browser.find_element(By.XPATH, "//button[text() = '가는 날']")
여기서 // -> html 문서 전체에서 찾겠다는뜻.
즉 "//button[text() = '가는날']" 은 텍스트 값이 '가는날'인 버튼 element를 찾는다는것.

일부 글자를 포함하는 경우 
browser.find_element(By.XPATH, '//i[contains(text(), "제주국제공항")]')

로딩이 되는경우 또는 코드가 너무 빨라 브라우저가 쫒아가지 못하는 경우 데이터를 가져올수 없다.
이것을 기다렸다가 실행하기 위한 방법.
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
두가지를 import 하고, 어떤 element가 나올때까지 기다리는데, 그 조건을 무엇으로 할지 결정한다.
WebDriverWait(browser, 30).until(EC.presence_of_element_located(
    (By.XPATH, '//div[@class="domestic_Flight__sK0eA result"]')))
최대 30초까지 기다리는데 , 어떤 엘리먼트가 표시 될때까지 기다려달라는것.
**클래스값 비교는 @를 사용한다.

다음과 같이 함수로 만들어 편하게 사용
def wait_until(xpath_str):
    WebDriverWait(browser, 30).until(EC.presence_of_element_located(
        (By.XPATH, xpath_str)))



넷플릭스 영화 검색 
movies = soup.find_all("div", attrs={"class": "ULeU3b neq64b"})
print(len(movies))
와 같이 실행하면 0이라 뜬다.
즉 영화를 검색해도 검색이 안되는데, html 파일을 직접 브라우저에서 열어보면 화면이 다르다.
접속하는 사용자의 header정보를 통해 넷플릭스에서 서로 다른 페이지를 리턴해주기 때문.

*** with open에서 파일 예쁘게 열기
f.write(soup.prettify())

유저 에이전트를 사용하여 접근을 하자

====================================================================

동적 페이지에 대한 웹 스크래핑
동적 페이지 - 사용자가 어떠한 동작을 했을 때 작동하는것.

requests를 사용하면 먼저뜨는것만 우리가 사용이 가능하다.
따라서 requests 대신 셀레니움을 사용한다.

셀레니움으로 페이지에 먼저 접속을 하고, 자바스크립트 코드를 실행시켜 스크롤을 밑으로 내린다.
browser.execute_script("window.scrollTo(0,1080)")
1080은 pc의 해상도 높이이며, 한페이지를 스크롤 다운시키는 것.

맨 밑으로 스크롤
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
현재 문서의 총 높이만큼 스크롤 내리는것.

** 높이가 변하지 않을때까지 내리기
interval = 2  -2초에 한번씩 스크롤 내리기
prev_height = browser.execute_script("return document.body.scrollHeight") -현재 문서 높이를 가져와 저장
반복 수행
while True:
    스크롤 아래로 내리기
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
    time.sleep(interval) -페이지 로딩 대기

    curr_height = browser.execute_script("return document.body.scrollHeight")
    if curr_height == prev_height:
        break

    prev_height = curr_height

스크롤을 맨 밑까지 내리고 원하는 정보를 찾자.

====================================================================

Headless Chrome
굳이 브라우저를 띄우지 않고 사용하기.

# 추가
options = webdriver.ChromeOptions()
options.headless = True
# 백그라운드에서 도는 브라우저의 크기
options.add_argument("window-size=640x480")

browser = webdriver.Chrome(
    "/Users/jeongju/Desktop/git/python_project/web_scraping/chromedriver", options=options)

** 스크린샷 찍기
browser.get_screenshot_as_file("google_movie.png")

주의할 점.
headless chrome 일때 따로 유저에이전트를 설정 안해주면 다음과 같이 뜬다.
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
AppleWebKit/537.36 (KHTML, like Gecko)
HeadlessChrome/101.0.4951.64 Safari/537.36
headlesschrome 라고 변해서 뜬다.
따라서 다음 문구를 추가
options.add_argument("user-agent=원래 유저 에이전트값")

====================================================================